{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.11/site-packages (2.2.1)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.17.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.11/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset class\n",
    "class CactusDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.classes = sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
    "        self.imgs = []\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            class_path = os.path.join(root, class_name)\n",
    "            for img_name in os.listdir(class_path):\n",
    "                if img_name.endswith(('.jpg', '.png')):\n",
    "                    self.imgs.append((os.path.join(class_path, img_name), class_idx))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, class_idx = self.imgs[idx]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor([[0, 0, img.size(1), img.size(2)]], dtype=torch.float32),  # Full image\n",
    "            \"labels\": torch.tensor([class_idx], dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx])\n",
    "        }\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and create train/validation split\n",
    "data_dir = '/Users/buzz/Documents/Cactus-main/Cactus Identification/Cactus Pictures'\n",
    "dataset = CactusDataset(data_dir, transforms=data_transforms)\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x))),\n",
    "    'val': DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "}\n",
    "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
    "class_names = dataset.classes\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained Faster R-CNN model and modify it\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, len(class_names))\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/19\n",
      "----------\n",
      "train Loss: 0.1457\n",
      "val Acc: 0.1183\n",
      "Epoch 1/19\n",
      "----------\n",
      "train Loss: 0.0791\n",
      "val Acc: 0.1720\n",
      "Epoch 2/19\n",
      "----------\n",
      "train Loss: 0.0691\n",
      "val Acc: 0.1720\n",
      "Epoch 3/19\n",
      "----------\n",
      "train Loss: 0.0640\n",
      "val Acc: 0.2043\n",
      "Epoch 4/19\n",
      "----------\n",
      "train Loss: 0.0633\n",
      "val Acc: 0.2849\n",
      "Epoch 5/19\n",
      "----------\n",
      "train Loss: 0.0597\n",
      "val Acc: 0.2903\n",
      "Epoch 6/19\n",
      "----------\n",
      "train Loss: 0.0557\n",
      "val Acc: 0.3172\n",
      "Epoch 7/19\n",
      "----------\n",
      "train Loss: 0.0489\n",
      "val Acc: 0.3333\n",
      "Epoch 8/19\n",
      "----------\n",
      "train Loss: 0.0472\n",
      "val Acc: 0.3441\n",
      "Epoch 9/19\n",
      "----------\n",
      "train Loss: 0.0476\n",
      "val Acc: 0.3441\n",
      "Epoch 10/19\n",
      "----------\n",
      "train Loss: 0.0474\n",
      "val Acc: 0.3387\n",
      "Epoch 11/19\n",
      "----------\n",
      "train Loss: 0.0471\n",
      "val Acc: 0.3226\n",
      "Epoch 12/19\n",
      "----------\n",
      "train Loss: 0.0468\n",
      "val Acc: 0.3387\n",
      "Epoch 13/19\n",
      "----------\n",
      "train Loss: 0.0461\n",
      "val Acc: 0.3817\n",
      "Epoch 14/19\n",
      "----------\n",
      "train Loss: 0.0456\n",
      "val Acc: 0.3656\n",
      "Epoch 15/19\n",
      "----------\n",
      "train Loss: 0.0454\n",
      "val Acc: 0.3495\n",
      "Epoch 16/19\n",
      "----------\n",
      "train Loss: 0.0459\n",
      "val Acc: 0.3495\n",
      "Epoch 17/19\n",
      "----------\n",
      "train Loss: 0.0459\n",
      "val Acc: 0.3710\n",
      "Epoch 18/19\n",
      "----------\n",
      "train Loss: 0.0451\n",
      "val Acc: 0.3656\n",
      "Epoch 19/19\n",
      "----------\n",
      "train Loss: 0.0454\n",
      "val Acc: 0.3656\n",
      "Training complete\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        for images, targets in dataloaders[phase]:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                if phase == 'train':\n",
    "                    loss_dict = model(images, targets)\n",
    "                    losses = sum(loss for loss in loss_dict.values())\n",
    "                    losses.backward()\n",
    "                    optimizer.step()\n",
    "                    running_loss += losses.item() * len(images)\n",
    "                else:\n",
    "                    outputs = model(images)\n",
    "                    preds = [output['labels'][0].item() for output in outputs]\n",
    "                    true_labels = [target['labels'][0].item() for target in targets]\n",
    "                    running_corrects += sum(p == t for p, t in zip(preds, true_labels))\n",
    "        \n",
    "        if phase == 'train':\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "        else:\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            print(f'{phase} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    exp_lr_scheduler.step()\n",
    "\n",
    "print('Training complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'faster_rcnn_cactus_classification.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images, targets in dataloaders['val']:\n",
    "        images = list(image.to(device) for image in images)\n",
    "        outputs = model(images)\n",
    "        preds = [output['labels'][0].item() for output in outputs]\n",
    "        labels = [target['labels'][0].item() for target in targets]\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate confusion matrix and plot it\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(classification_report(all_labels, all_preds, target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Perform inference\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Extract predictions\n",
    "            preds = [output['labels'].cpu().numpy() for output in outputs]\n",
    "            true_labels = [target['labels'].cpu().numpy() for target in targets]\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(true_labels)\n",
    "    \n",
    "    return all_preds, all_targets\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "all_preds, all_targets = evaluate_model(model, dataloaders['val'], device)\n",
    "inference_time = (time.time() - start_time) / len(dataloaders['val'])\n",
    "\n",
    "# Flatten lists of arrays\n",
    "flatten_preds = [item for sublist in all_preds for item in sublist]\n",
    "flatten_targets = [item for sublist in all_targets for item in sublist]\n",
    "\n",
    "# Convert to binary classification for metrics\n",
    "# Note: This is a simplified approach for multi-class, adapt as needed for your use case\n",
    "y_pred = [1 if p in flatten_preds else 0 for p in flatten_preds]\n",
    "y_true = [1 if t in flatten_targets else 0 for t in flatten_targets]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='weighted')\n",
    "recall = recall_score(y_true, y_pred, average='weighted')\n",
    "f1 = f1_score(y_true, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {precision * 100:.2f}%\")\n",
    "print(f\"Recall: {recall * 100:.2f}%\")\n",
    "print(f\"F1 Score: {f1 * 100:.2f}%\")\n",
    "print(f\"Average Inference Time per Batch: {inference_time:.6f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
